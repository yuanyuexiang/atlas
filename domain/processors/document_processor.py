"""
æ–‡æ¡£å¤„ç†æœåŠ¡ - è´Ÿè´£æ–‡æ¡£åŠ è½½ã€åˆ†å‰²å’Œé¢„å¤„ç†
èŒè´£ï¼šæ–‡æ¡£æ ¼å¼è½¬æ¢ã€æ–‡æœ¬åˆ†å‰²ã€å†…å®¹è¿‡æ»¤
"""
import os
from typing import List, Tuple
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document


class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†æœåŠ¡"""
    
    def __init__(
        self,
        chunk_size: int = 800,
        chunk_overlap: int = 120,
        max_chunk_length: int = 250
    ):
        """
        åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨
        
        Args:
            chunk_size: æ–‡æœ¬åˆ†å—å¤§å°
            chunk_overlap: æ–‡æœ¬å—é‡å å¤§å°
            max_chunk_length: å•ä¸ªæ–‡æœ¬å—æœ€å¤§é•¿åº¦ï¼ˆå­—ç¬¦æ•°ï¼‰
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.max_chunk_length = max_chunk_length
        
        # æ–‡æœ¬åˆ†å‰²å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            add_start_index=True,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "ï¼Œ", " "]
        )
    
    def load_document(self, file_path: str) -> List[Document]:
        """
        åŠ è½½æ–‡æ¡£
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            List[Document]: åŠ è½½çš„æ–‡æ¡£åˆ—è¡¨
            
        Raises:
            FileNotFoundError: æ–‡ä»¶ä¸å­˜åœ¨
            ValueError: ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹
        """
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        filename = os.path.basename(file_path)
        
        # PDF æ–‡ä»¶
        if file_path.endswith('.pdf'):
            loader = PyPDFLoader(file_path)
            docs = loader.load()
            print(f"  åŠ è½½ PDF: {len(docs)} é¡µ")
            return docs
        
        # æ–‡æœ¬æ–‡ä»¶
        elif file_path.endswith(('.txt', '.md')):
            docs = self._load_text_file(file_path)
            print(f"  åŠ è½½æ–‡æœ¬æ–‡ä»¶: {len(docs)} ä¸ªæ–‡æ¡£")
            return docs
        
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {filename}")
    
    def _load_text_file(self, file_path: str) -> List[Document]:
        """
        å°è¯•å¤šç§ç¼–ç åŠ è½½æ–‡æœ¬æ–‡ä»¶
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            List[Document]: åŠ è½½çš„æ–‡æ¡£
            
        Raises:
            ValueError: æ‰€æœ‰ç¼–ç éƒ½å¤±è´¥
        """
        encodings = ['utf-8', 'gbk', 'gb2312', 'gb18030', 'latin-1']
        
        for encoding in encodings:
            try:
                loader = TextLoader(file_path, encoding=encoding)
                docs = loader.load()
                print(f"  ä½¿ç”¨ {encoding} ç¼–ç åŠ è½½æˆåŠŸ")
                return docs
            except Exception:
                continue
        
        raise ValueError(f"æ— æ³•åŠ è½½æ–‡ä»¶ï¼Œå°è¯•äº†æ‰€æœ‰ç¼–ç : {encodings}")
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """
        åˆ†å‰²æ–‡æ¡£ä¸ºæ–‡æœ¬å—
        
        Args:
            documents: åŸå§‹æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            List[Document]: åˆ†å‰²åçš„æ–‡æœ¬å—åˆ—è¡¨
        """
        splits = self.text_splitter.split_documents(documents)
        print(f"  åˆ†å‰²ä¸º {len(splits)} ä¸ªæ–‡æœ¬å—")
        return splits
    
    def filter_and_truncate(self, documents: List[Document]) -> List[Document]:
        """
        è¿‡æ»¤å’Œæˆªæ–­è¿‡é•¿çš„æ–‡æœ¬å—
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            List[Document]: å¤„ç†åçš„æ–‡æ¡£åˆ—è¡¨
        """
        filtered = []
        for doc in documents:
            content = doc.page_content
            if len(content) > self.max_chunk_length:
                doc.page_content = content[:self.max_chunk_length] + "..."
            filtered.append(doc)
        
        print(f"  è¿‡æ»¤åä¿ç•™ {len(filtered)} ä¸ªæ–‡æœ¬å—")
        return filtered
    
    def add_metadata(
        self,
        documents: List[Document],
        file_id: str,
        filename: str,
        agent_name: str
    ) -> List[Document]:
        """
        ä¸ºæ–‡æ¡£æ·»åŠ å…ƒæ•°æ®
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            file_id: æ–‡ä»¶ ID
            filename: æ–‡ä»¶å
            agent_name: æ™ºèƒ½ä½“åç§°
            
        Returns:
            List[Document]: æ·»åŠ å…ƒæ•°æ®åçš„æ–‡æ¡£åˆ—è¡¨
        """
        for doc in documents:
            doc.metadata.update({
                'file_id': file_id,
                'filename': filename,
                'agent_name': agent_name
            })
        return documents
    
    def process_file(
        self,
        file_path: str,
        file_id: str,
        filename: str,
        agent_name: str
    ) -> Tuple[List[Document], dict]:
        """
        å®Œæ•´çš„æ–‡æ¡£å¤„ç†æµç¨‹
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            file_id: æ–‡ä»¶ ID
            filename: æ–‡ä»¶å
            agent_name: æ™ºèƒ½ä½“åç§°
            
        Returns:
            Tuple[List[Document], dict]: (å¤„ç†åçš„æ–‡æ¡£åˆ—è¡¨, å¤„ç†ç»Ÿè®¡ä¿¡æ¯)
            
        Raises:
            FileNotFoundError: æ–‡ä»¶ä¸å­˜åœ¨
            ValueError: æ–‡ä»¶å¤„ç†é”™è¯¯
        """
        print(f"ğŸ“„ å¤„ç†æ–‡ä»¶: {filename}")
        
        # 1. åŠ è½½æ–‡æ¡£
        docs = self.load_document(file_path)
        
        # 2. åˆ†å‰²æ–‡æ¡£
        splits = self.split_documents(docs)
        
        # 3. è¿‡æ»¤å’Œæˆªæ–­
        filtered = self.filter_and_truncate(splits)
        
        # 4. æ·»åŠ å…ƒæ•°æ®
        processed = self.add_metadata(filtered, file_id, filename, agent_name)
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'original_docs': len(docs),
            'splits': len(splits),
            'filtered': len(filtered),
            'final': len(processed)
        }
        
        return processed, stats


# å…¨å±€å•ä¾‹
_document_processor = None


def get_document_processor() -> DocumentProcessor:
    """è·å–æ–‡æ¡£å¤„ç†å™¨å•ä¾‹"""
    global _document_processor
    if _document_processor is None:
        _document_processor = DocumentProcessor()
    return _document_processor
